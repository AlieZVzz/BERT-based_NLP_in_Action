{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to distilbert-base-uncased-finetuned-sst-2-english (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'NEGATIVE', 'score': 0.9878195524215698}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import *\n",
    "\n",
    "nlp_sentence_classif = pipeline('sentiment-analysis')\n",
    "print(nlp_sentence_classif(\n",
    "    'Im not going to lie and say I dont watch the show--I do BUT it has a lot and a lot of flaws 1 The Boarding School is perfect The drama is at a minimum Everyone is so nice to each other you know Lets give that a reality check Its IMPOSSIBLE that ANY school is perfect like PCA Free laptops for everyone Big dorm rooms Mini fridges If there was a school like that in real life almost nobody there would be a virgin for one Two everyone there is so rich and its weird how nobody has anything stolen yet 2 Characters really unrealistic First things first who in theyre right minds talk like they do They talk like a perfect teenager would Secondly Logan ReeseMatthew Underwood is an extremely rich boy hot teenage boy My question is why isnt almost ever girl in that school all over him? Hes rich and hot now a days all those girls would be after him even if he was a jerk Also Chase is the most stupidest person ever He is this shy teenager who claims to not be in love with Zoey and over-reacts to everything that involves Zoey She must be BLIND not to see him in love with her  Come on Nick I know you can do better than THAT Please'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['bert-base-uncased', 'bert-large-uncased', 'bert-base-cased', 'bert-large-cased', 'bert-base-multilingual-uncased', 'bert-base-multilingual-cased', 'bert-base-chinese', 'bert-base-german-cased', 'bert-large-uncased-whole-word-masking', 'bert-large-cased-whole-word-masking', 'bert-large-uncased-whole-word-masking-finetuned-squad', 'bert-large-cased-whole-word-masking-finetuned-squad', 'bert-base-cased-finetuned-mrpc', 'bert-base-german-dbmdz-cased', 'bert-base-german-dbmdz-uncased', 'cl-tohoku/bert-base-japanese', 'cl-tohoku/bert-base-japanese-whole-word-masking', 'cl-tohoku/bert-base-japanese-char', 'cl-tohoku/bert-base-japanese-char-whole-word-masking', 'TurkuNLP/bert-base-finnish-cased-v1', 'TurkuNLP/bert-base-finnish-uncased-v1', 'wietsedv/bert-base-dutch-cased']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BERT_PRETRAINED_MODEL_ARCHIVE_LIST\n",
    "print(BERT_PRETRAINED_MODEL_ARCHIVE_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[CLS]',\n",
       " 'who',\n",
       " 'is',\n",
       " 'li',\n",
       " 'jin',\n",
       " '##hong',\n",
       " '?',\n",
       " '[SEP]',\n",
       " 'li',\n",
       " 'jin',\n",
       " '##hong',\n",
       " 'is',\n",
       " 'a',\n",
       " 'programmer',\n",
       " '[SEP]']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "text = '[CLS] who is Li Jinhong? [SEP] Li Jinhong is a programmer [SEP]'\n",
    "tokenized_text = tokenizer.tokenize(text)\n",
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  101,  2040,  2003,  5622,  9743, 19991,  1029,   102,   103,  9743,\n",
       "         19991,  2003,  1037, 20273,   102]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masked_index  = 8\n",
    "tokenized_text[masked_index] = '[MASK]'\n",
    "indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "tokens_tensor = torch.tensor([indexed_tokens])\n",
    "tokens_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0')\n",
    "model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "model.to(device)\n",
    "segment_ids = [0,0,0,0,0,0,0,0,1,1,1,1,1,1,1]\n",
    "segment_tensors = torch.tensor([segment_ids]).to(device)\n",
    "tokens_tensor = tokens_tensor.to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model(tokens_tensor, token_type_ids = segment_tensors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = outputs[0]\n",
    "predicted_index = torch.argmax(predictions[0, masked_index]).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'li'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token = tokenizer.convert_ids_to_tokens([predicted_index])[0]\n",
    "predicted_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bos_token\n",
      "eos_token\n",
      "unk_token\n",
      "sep_token\n",
      "pad_token\n",
      "cls_token\n",
      "mask_token\n",
      "additional_special_tokens\n"
     ]
    }
   ],
   "source": [
    "for tokerstr in tokenizer.SPECIAL_TOKENS_ATTRIBUTES:\n",
    "    print(tokerstr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = []\n",
    "data = open('../dataset/train.txt', 'r', encoding='utf-8')\n",
    "for i in data.readlines():\n",
    "    text_list.append(i.split('\\t')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8038"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy import displacy\n",
    "from pathlib import Path\n",
    "parser = spacy.load('en_core_web_sm')\n",
    "doc = 'We focus on the research and education of AI technology'\n",
    "doc = parser(doc)\n",
    "svg = displacy.render(doc, style='dep', jupyter=False)\n",
    "output_path = Path('./dependency_plot.svg')\n",
    "output_path.open('w', encoding='utf-8').write(svg)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "79f39d06ccac4f0a6b3d11477479a3d7a8039e2bca6f83223d20136807394865"
  },
  "kernelspec": {
   "display_name": "Python 3.7.12 64-bit ('torch': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
